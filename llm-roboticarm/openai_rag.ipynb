{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your OpenAI API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jongh\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\jongh\\anaconda3\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the embedding model with your OpenAI API key\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=\"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\")\n",
    "\n",
    "# Initialize the language model with your OpenAI API key\n",
    "llm = ChatOpenAI(model=\"gpt-4\", openai_api_key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhancing Human-Robot Collaborative Assembly in Manufacturing Systems Using Large Language Models\n",
      "\n",
      "Jonghan Lim1, Sujani Patel2, Alex Evans3, John Pimley1, Yifei Li2, and Ilya Kovalenko1,2\n",
      "\n",
      "Abstract— The development of human-robot collaboration has the ability to improve manufacturing system performance by leveraging the unique strengths of both humans and robots. On the shop floor, human operators contribute with their adapt- ability and flexibility in dynamic situations, while robots provide precision and the ability to perform repetitive tasks. However, the communication gap between human operators and robots limits the collaboration and coordination of human-robot teams in manufacturing systems. Our research presents a human- robot collaborative assembly framework that utilizes a large language model for enhancing communication in manufac- turing environments. The framework facilitates human-robot communication by integrating voice commands through natural language for task management. A case study for an assembly task demonstrates the framework’s ability to process natural language inputs and address real-time assembly challenges, emphasizing adaptability to language variation and efficiency in error resolution. The results suggest that large language models have the potential to improve human-robot interaction for collaborative manufacturing assembly applications.\n",
      "\n",
      "I. INTRODUCTION Advances in robotics technology have significantly en- hanced manufacturing efficiency, resulting in cost reductions and increased productivity [1], [2]. While robots can execute rapid, accurate, and repetitive tasks that demand heavy-duty efforts in manufacturing settings, they lack the capability for adaptation and versatility of human operators [2]. There- fore, the importance of Human-Robot Collaboration (HRC) is growing as humans and robots complement each other skills and capabilities. HRC refers to the interaction and cooperation between human operators and robotic systems within shared workspace [3]. Prior works have used HRC frameworks to improve ergonomics in manufacturing settings and safe human-robot interaction without compromising pro- ductivity [4], [5]. Transitioning from tasks involving large components, such as loading, placing, and unloading, to the complex assembly of smaller components such as printed circuit boards, the collaboration between human operators and robots significantly enhances both the efficiency and safety of production lines [6], [7].\n",
      "\n",
      "Further advancing HRC in manufacturing systems by improving the interaction of human operators and robots presents significant challenges. Specifically, interaction with robots induces psychological stress and tension among opera- tors due to language barrier [8]. In contemporary manufactur- ing systems, human operators require extensive pre-service\n",
      "\n",
      "The authors [1] are with the Department of Industrial and Manufacturing, [2] are with the Department of Mechanical Engineering, and [3] are with the Department of Computer Science and Engineering, at the Pennsylvania State University, State College, USA (e-mail: {jxl567; spp22; ade5221; jap6581; ybl5717; iqk5135}@psu.edu).\n",
      "\n",
      "training and complex code development to ensure accurate and safe production with the robots [9]. These difficulties highlight the need to develop human-robot communication systems that enable interaction between humans and robots without extensive robotics training (C1). Another challenge involves the need for greater flexibility and adaptability in human-robot interaction. As the number of interactions between humans and robots on the shop floor increases, there is a higher chance of encountering unexpected changes and errors. Thus, HRC needs to be more adaptable to changes and errors during the manufacturing assembly process (C2). Ad- ditionally, human-robot collaborative assembly applications must integrate advanced technologies with a human-centric design to improve communication and usability (C3).\n",
      "\n",
      "Large Language Models (LLMs) have recently been in- troduced in the AI community to enhance natural language understanding and generation capabilities. These can be extended to improve human-robot interaction in the man- ufacturing facility. Models such as OpenAI’s GPT-3 [10] and GPT-4 [11], have shown proficiency in processing, understanding, and communicating in natural language. The integration of LLMs facilitates natural language communi- cation between humans and robots. Using voice interaction for this communication enhances collaboration and operator safety in dynamic workspaces. The main contributions of this work are: (1) the use of LLMs for interpreting natural language to allow operators to coordinate with robotic arms, (2) a framework for system integration of voice commands, robotic arms, and vision systems to facilitate HRC in an assembly task, enhancing operational flexibility, and (3) an adaptation to task errors and obstacles through human-robot communication in a manufacturing environment.\n",
      "\n",
      "The rest of the manuscript is organized as follows. Sec- tion II reviews related work and identifies gaps in HRC for manufacturing assembly. Section III describes the proposed integrates LLMs with voice commands, framework that robotic arm, and sensors. Section IV showcases a case study and evaluates the framework’s performance. Finally, Section V provides conclusions and future work.\n",
      "\n",
      "II. BACKGROUND\n",
      "\n",
      "A. Human-Robot Collaboration in Manufacturing\n",
      "\n",
      "Recently, a wide range of methods has been developed in the field of HRC to improve the safety and efficiency of interactions between humans and robots. For instance, Fer- nandez et al. [12] developed a dual-arm robotic system with multisensor capabilities for safe and efficient collaboration, integrating gesture recognition. Additionally, Wei et al. [13]\n",
      "\n",
      "gather information on human operators and the environment using RGB-D videos and predict human intentions based on deep-learning methods.\n",
      "\n",
      "Some approaches have utilized multi-modal strategies, including natural language, to improve communication for HRC in manufacturing. For example, Liu et al. [14] focus on integrating various modalities, including speech commands, hand motions, and body motions to improve HRC. While it addresses speech command recognition using deep learning models, the interaction is limited to basic speech command recognition and does not focus on context-aware communica- tion. Additionally, Wang et al. [15], [16] employ a teaching- learning model that includes natural language instructions to predict human intentions and facilitate collaboration. While this model uses natural language for multimodal processing, it does not emphasize utilizing language variations in inter- action.\n",
      "\n",
      "Previous research have introduced methods using envi- ronmental data to enhance safety and efficiency and natural language to improve HRC in manufacturing. However, there is limited research on human-robot collaborative assembly that effectively integrates natural language capabilities for context-aware communication and handling language vari- ations. We aim to integrate an LLM-based approach to improve communication between humans and robots. Our approach is an initial step towards enhancing human-robot interaction by combining existing technologies, such as com- puter vision and LLM, to leverage human flexibility and robot precision in manufacturing.\n",
      "\n",
      "B. Large Language Models for Robots\n",
      "\n",
      "Recent research has explored integrating LLMs in robotic applications to enhance human-robot interaction and robot functionalities [17], [18], [19], [20], [21], [22]. This includes converting high-level human instructions into tasks robots can execute. Huang et al. [17] used LLMs, such as GPT-3 and CODEX, to convert high-level human instructions into executable actions for robots. Similarly, Singh et al. [18] developed a method called ProgPrompt, which generates Python code for task planning that accounts for environmen- tal conditions and pre-programmed actions. Furthermore, Lin et al. [19] developed a framework for taking natural language instructions and then developing task and motion plans. Other works in robotics have utilized LLMs to maximize the robot’s capability to comprehend and interact with its environment to resolve errors. Mees et al [20] combined natural language input with visual information to enhance task completion ability. Zhang et al. [21] utilizes LLM to address multi-agent cooperation challenges, resolving issues related to decentralized control, raw sensory data process- ing, and efficient task execution in embodied environments. KnowNo [22] improved robot autonomy and efficiency by guiding robots in seeking human help when faced with uncertainty.\n",
      "\n",
      "These studies show how integrating LLMs into robotic applications improves human-robot interaction and robot capabilities. However, existing studies have not addressed\n",
      "\n",
      "common challenges found in the manufacturing environment, such as matching natural language to structured task se- quences and managing the timing of commands (e.g., iden- tifying when to request operator assistance in the process). We aim to bridge the gap by improving the integration of natural language for human-robot interaction for an assembly process through an LLM-based framework.\n",
      "\n",
      "III. FRAMEWORK\n",
      "\n",
      "The proposed framework, presented in Figure 1, is devel- oped for a human-robot collaborative assembly within the manufacturing environment. The framework is designed to facilitate interaction between a human operator and a robot for an assembly process.\n",
      "\n",
      "In this framework, a human can initialize and provide a set of tasks T = {t1, t2, . . . , tn} as the objectives to be achieved by the robot. While T is predefined, the system can be extended to integrate new tasks based on human input. The robot’s capabilities C = {c1, c2, . . . , cl} represent the specific actions programmed to perform within an assembly process. If the task matches the robot’s capabilities, the robot should execute the specified task ti. The task, ti, is broken down into subtasks (ti1, ti2, . . . , tik) that outline the sequential steps for completion. The framework has to track the last completed subtask, denoted as tic, where 1 ≤ c ≤ k indicates the completed process within ti1, ti2, . . . , tik. When an error is resolved by a human operator, the robot resumes from tic+1, ensuring continuous task progression. To enable this human-robot collaborative assembly process, this framework is structured into two primary layers: the physical layer and the virtual layer, bridging the gap between human inputs and robot actions.\n",
      "\n",
      "A. Physical Layer\n",
      "\n",
      "interaction within a shared space, based on data from the virtual layer. The physical consists of three primary components: human commands, robot actions, and sensor data. Human com- mands involve the operator providing voice instructions to control robot actions. Robot actions are based on a predefined set of T . Sensor data is utilized to monitor environmental conditions. This data ensures the action adapts to changes in the workspace (e.g. position, orientation, or availability of parts).\n",
      "\n",
      "The physical\n",
      "\n",
      "layer\n",
      "\n",
      "facilitates human-robot\n",
      "\n",
      "When a predefined event or error ei ∈ E is detected, the robot alerts the human operator using a communication protocol. The LLM module converts ei into a natural lan- guage message Mei(ti), delivered via text-to-speech tech- nology. Different instances of ei can lead to varied message Mei(ti) depending on the task ti error. After receiving and understanding Mei(ti), the human operator resolves the corresponding ei and commands the robot to resume ti.\n",
      "\n",
      "B. Virtual Layer\n",
      "\n",
      "The virtual layer serves to facilitate communication. This layer stores the system’s functionality to enable interaction between human commands and robot actions. The virtual\n",
      "\n",
      "Fig. 1: Human-Robot Collaborative Assembly Framework Using LLM\n",
      "\n",
      "layer of the system consists of two main agents: a human agent and a robot agent.\n",
      "\n",
      "1) Human Agent: We have integrated a human agent, often referred to as a human command interpreter, to enhance the human-robot interaction based on the approach by Zhang is to process et al. [23]. The goal of the human agent the human commands in voice instructions into a text that the robot agent can understand. This agent ensures that the human intentions are transferred to the robot without manual programming. The human agent contains an Au- tomatic Speech Recognition (ASR) module responsible for processing the human command voice data, which converts this data into text that the robot agent can process. The communication module in the human agent serves as a bridge, transmitting commands and information to the robot. 2) Robot Agent: The robot agent interprets and executes the set of manufacturing tasks T based on the voice com- mands received from the human operator. This process is facilitated by various functional modules:\n",
      "\n",
      "Initialization Module: The initialization of the robot agent, provides basic operational guidelines and task execution protocols. This process involves programming the robot with instructions to perform tasks T within its capabilities C and to communicate errors or request assistance from human operators when necessary. The following initial prompt is given to the robot agent: collaborative assembly system designed to assist\n",
      "\n",
      "in tasks and respond to commands. Upon receiving\n",
      "\n",
      "a request within your capability range, execute\n",
      "\n",
      "the service. In the event of encountering errors,\n",
      "\n",
      "request assistance from a human operator for error\n",
      "\n",
      "correction, providing clear and understandable\n",
      "\n",
      "This initialization prompt defines the robot agent’s role in responding to commands. It also establishes a pro- tocol for seeking assistance and communicating errors to human operators for task completion. Following the initial setup of the robot agent, specific capabilities C are defined, such as specific assembly tasks.\n",
      "\n",
      "LLM Module: An LLM interprets human commands into task ti ∈ T using initialization prompts and detailed functional information. Human commands and func- tional capabilities C including the detailed functional information of C are put into the LLM. The LLM then analyzes the language of the human commands and understands the context based on the provided information, such as the last subtask completed and the error encountered, enabling it to detect the next task automatically. This process allows the system to match the command with the predefined functional capabilities C, ensuring the intended task is executed. The LLM module also communicates any detected issues from the task control module into understandable natural language for the human operators, facilitating HRC for error resolution. While static error messages do not cap- ture the context of an error event, processing these with LLM adapts to the situation, providing more actionable information to human operators. This approach helps human operators understand the next steps based on the robot’s guidance.\n",
      "\n",
      "Sensor Module: The sensor module processes data from the physical layer such as position, orientation, and part availability. This information is utilized for adjust- ing robot actions required for executing manufacturing tasks T . For example, the module provides component positions and orientations for the task control module. This enables accurate robotic adjustments for assembly\n",
      "\n",
      "explanations.\"\n",
      "\n",
      "Fig. 2: Sequence Diagram for Human-Robot Collaborative Assembly in Manufacturing Systems\n",
      "\n",
      "or initiates error notifications for missing components, ensuring effective task execution and error management. • Task Control Module: The task control module directs robot actions to fulfill task ti within the set T , based on the functional capabilities C. The task control mod- ule adjusts robot actions to environmental conditions this obtained from the sensor module. Additionally, module plays a role in managing errors by verifying sensor data and communicating detected issues through the LLM module to facilitate resolution by the human operator.\n",
      "\n",
      "C. Human-Robot Collaborative Assembly Workflow\n",
      "\n",
      "The overall workflow is shown in a sequence diagram in Figure 2, illustrating the human-robot collaborative assembly process. The diagram depicts how voice commands from the human operator are processed by the LLM module to guide robot actions.\n",
      "\n",
      "The process starts with the operator giving a voice com- mand, converted by the LLM module into a discrete set of tasks T for the robot. The robot then requests sensor data to execute ti. If the data is valid, the robot proceeds to execute the assigned ti. The sensor module determines data validity by comparing detected parameters to predefined criteria. Successful execution results in a completion message Mc(ti) to the operator via the LLM Module.\n",
      "\n",
      "the robot generates an error message Mei (ti) via the LLM module, aiming to inform the human operator of the specific error and its occurrence within subtask tic+1 for efficient resolution. Following the error identification and correction by the human operator, a new command by the human operator is issued to the robot. The robot then resumes task execution at ti, starting from the interrupted subtask tic+1, based on the new sensor data. This procedure is repeated until ti is completed.\n",
      "\n",
      "If the data is invalid, or ti has any errors,\n",
      "\n",
      "IV. CASE STUDY\n",
      "\n",
      "The proposed framework was tested in a manufacturing assembly manufacturing cell. The goal of the manufacturing\n",
      "\n",
      "Fig. 3: Cable Shark Assembly\n",
      "\n",
      "cell is to assemble a cable shark product. The cable shark assembly contains (1) a housing, (2) a wedge, (3) a spring, (4) and an end cap. The exploded cable shark assembly and the final assembled product are shown in Figure 3. This assembly process task t1 include four sequential subtasks: housing assembly t11, wedge assembly t12, spring assembly t13, and end cap assembly t14.\n",
      "\n",
      "The physical setup uses a Six Degrees of Freedom (6DOF) UFactory xArm with a UFactory 2-finger gripper as the end effector [24]. The base of the robotic arm remains fixed at the center, accompanied by a mat on one side, while the parts are assembled on the adjacent side. Two Intel RealSense Depth Camera D435 cameras were used as visual sensors in the surrounding area, one focused on the mat area for pre-assembly component placement, and the other camera on the assembling process. The conversion between camera and robot arm coordinates is accomplished through the coordinate transformation. We used a fixed z-axis value for the robotic arm to pick up camera-identified parts. In future work, we aim to make this z-axis value adaptable.\n",
      "\n",
      "A. LLM and ASR Module\n",
      "\n",
      "This section outlines how LLM and the ASR module, were implemented within the system. The communication aspect within the system is enabled by OpenAI’s speech-to- text and text-to-speech models. The OpenAI’s transcription model ‘whisper1’ [25] is implemented to transform speech- to-text, ensuring that human voice instructions are accurately captured. To enable the communication protocol to provide a verbal response from the robot, OpenAI’s text-to-speech model ‘tts-1’ [26] is utilized to generate audio responses. This approach enhances and allows for a clear exchange of information between the human and the robot.\n",
      "\n",
      "The LLM module leverages OpenAI’s pre-trained GPT- 4.0 [11]. The LLM module converts instructions from the human operator into tasks T executable by the robot, uti- lizing its capabilities C through OpenAI’s function calling feature [27]. This module ensures that the robot actions match the human operator commands, allowing for adaptive and responsive task management within the manufacturing environment.\n",
      "\n",
      "B. Sensor Module: Vision System\n",
      "\n",
      "We incorporated a vision system as our sensor module. During the collaborative assembly process, the vision sys- tem provides feedback on environmental data to the task control module. This enables precise object detection and identification of object orientation, facilitating task execution\n",
      "\n",
      "Fig. 4: Feature Extraction Method with the Vision System\n",
      "\n",
      "and error management. To enable object detection, Yolov5, a computer vision model, is utilized [28]. Custom Yolov5 models are trained using a dataset of images of individual parts (i.e. housing, wedge, spring, end cap), with manual bounding box annotations around each part in the images. The vision system these annotations to identify the parts, as shown in Figure 4. If a part is detected, the coordinates of the top-left and bottom-right corners of the bounding box are retrievable. Once these coordinates are obtained, the x and y coordinates of these points are utilized to determine the object’s midpoint. This midpoint serves as the target point for the robotic arm to pick up the part. Following the successful identification, the valid sensor data is transferred to the task control module for guiding robot action. For inaccurately detected parts, the sensor module transfers invalid data to the task control module to notify the human operator via LLM.\n",
      "\n",
      "C. Task Control Module: Assembly Task\n",
      "\n",
      "The Task Control Module executes tasks from human commands interpreted by the LLM and manages errors in a human-robot collaborative assembly system. The task control module verifies sensor data from the vision system. For instance, if the data is valid, the task control module proceeds; if not, the task control module sends error details (e.g. missing component) to the LLM module to convert the information into natural language. This process activates the communication protocol, allowing the human operator to recognize and address the error. To enable the assembly process, the task control module integrates various function- alities, including coordinate transformation and position and rotation control, based on the sensor data obtained from the vision system. These functions enable the execution of tasks interpreted by the LLM. The orientation of detected objects is determined from the bounding box dimensions, assuming the object’s angle is perpendicular or parallel to the x-axis of the camera vision. The cable shark assembly process is shown in Figure 5.\n",
      "\n",
      "D. Case Study Results\n",
      "\n",
      "The proposed framework was integrated into the assembly system to study the effect of integrating LLMs with a knowledgeable operator1. The operator knew the assembly process and was able to converse. Three scenarios were used to evaluate the proposed framework: Scenario 1: The system detects an overlap of the housing components and requests human intervention.\n",
      "\n",
      "1See supplementary video file for a demonstration of the human-robot\n",
      "\n",
      "collaborative assembly.\n",
      "\n",
      "Fig. 5: Cable Shark Assembly Process\n",
      "\n",
      "Scenario 2: If a wedge component is incorrectly assembled, the robot halts and requests human correction. Scenario 3: A missing spring component is detected and the robot requests the human operator to place the spring component.\n",
      "\n",
      "Based on these scenarios, the evaluation assessed the sys- tem’s proficiency in interpreting and performing tasks from human commands, considering the linguistic and human diversity. Variations in phrasing and terminology across dif- ferent commands were cataloged, as presented in Table I, to validate the system’s adaptability to diverse linguistic inputs. Instructions were classified into three distinct categories: specific, moderately specific, and least specific. Five unique variations of each instruction type were provided across tasks. To ensure reliability, each language three distinct variation scenario presented in Table I is conducted with three repetitions.\n",
      "\n",
      "The results highlight the effectiveness of integrating LLM into the human-robot collaborative assembly framework. Figure 6 showcases the human-robot communication using the vision system, outlined in Section IV-B, to ensure task completion. In scenario 1, the robot detects overlapping components which signal to human operators for help. Upon resolution, the human operator prompts task continuation. Similarly in scenario 2, the robot identifies a misassembled wedge, notifying the human operator, and then the human commands the robot to continue after manually assembling\n",
      "\n",
      "TABLE I: Language Variations for Task Instructions\n",
      "\n",
      "Scenario 1\n",
      "\n",
      "Scenario 1: Component Overlap\n",
      "\n",
      "Scenario 2: Incorrectly Assembled Part\n",
      "\n",
      "Scenario 3: Missing Component\n",
      "\n",
      "Instruction Type Specific Moderately Specific Least Specific Specific Moderately Specific Least Specific Specific Moderately Specific Least Specific\n",
      "\n",
      "Instruction [\"Overlap resolved. Proceed with the task.\"], [\"Problem is...\"] [\"I’ve placed the components correctly.\"], [\"I’ve sorted out the...\"] [\"Fixed.\"], [\"Done.\"], [\"Completed.\"], [\"Handled.\"], [\"Adjusted.\"] [\"Correction is made. Resume the task.\"], [\"The wedge is...\"] [\"I’ve fixed the issue with the wedge.\"], [\"I’ve placed the wedge...\"] [\"Fixed.\"], [\"Done.\"], [\"Addressed.\"], [\"All set.\"], [\"Under control.\"] [\"I’ve placed the spring component. Please proceed.\"].[\"Spring...\"] [\"I’ve fixed the issue with the spring.\"], [\"The spring component...\"] [\"Fixed.\"], [\"Done.\"], [\"Managed.\"], [\"Handled\"], [\"Settled.\"]\n",
      "\n",
      "TABLE II: Success Rates for Language Variations\n",
      "\n",
      "E. Case Study Discussion and Limitations\n",
      "\n",
      "Scenario\n",
      "\n",
      "Scenario 1: Component Overlap\n",
      "\n",
      "Scenario 2: Incorrectly Assembled Part\n",
      "\n",
      "Scenario 3: Missing Component\n",
      "\n",
      "Instruction Category Specific Moderately Specific Least Specific Specific Moderately Specific Least Specific Specific Moderately Specific Least Specific\n",
      "\n",
      "Success Rate 100% 73% 27% 93% 87% 53% 100% 67% 27%\n",
      "\n",
      "The case study evaluated the system’s ability to facilitate collaboration between humans and robots and showcases how the integration of LLMs can lead to efficient and flexible manufacturing processes. The results show that as instruc- tions become less specific, robot performance significantly decreases, indicating the need for well-defined commands. For example, the ambiguous command \"Correction is made. Resume the operations.\" failed due to lack of context and explicit task reference. These results highlight the limita- tions of the proposed framework and suggest an area for improvement. One such improvement involves introducing feedback mechanisms that allow the system to ask for clari- fication when instructions are unclear, refining the interaction between humans and robots. Another solution is to fine- tune the LLM [29] or integrate a knowledge base [30] with specific manufacturing processes to improve its handling of instructions and their correct responses. Commands contain- ing keywords like ’fixed’ or ’addressed’ had higher success rates, emphasizing the importance of initialization protocol. The protocol informs the robot that the human’s role is to correct errors to resume the assembly task. The case study found that a clear initialization process is crucial for the robot to understand its role and functions within the system.\n",
      "\n",
      "Fig. 6: Case Study Communication Results for Each Scenario\n",
      "\n",
      "the wedge. For Scenario 3, the robot flags a missing spring component, requesting human intervention, and the human instructs the robot to resume the assembly once the issue is fixed.\n",
      "\n",
      "The evaluation assessed the system’s capability to un- derstand and execute commands with varied language ex- pressions. Success rate refers to the percentage of the robot correctly understanding and executing the given instruction without requiring additional clarification. For tasks with specific instructions, success rates were high, averaging approximately 98%. The rates decreased to 76% with mod- erately specific instructions and dropped further to 36% with least specific instructions. The success rates are detailed in Table II. This data suggests a positive correlation between instruction type and task execution success.\n",
      "\n",
      "There were also limitations in the case study and eval- uation of the proposed framework. Specifically, we only evaluated a limited range of commands from the operator, rather than assessing the overall system capabilities. These commands were related to predefined assembly scenarios and did not include other interactions, such as human in- terruptions and task-irrelevant questions from the human. While LLM-generated error messages were understandable for knowledgeable operators, there is a risk of low-quality messages for less experienced operators or in varied contexts. The case study also did not study the variability in the operator’s knowledge, as the framework assumed operators would provide relevant instructions to the assembly task. Furthermore, the operator was not allowed to change the defined tasks, e.g., variations in positioning or task order.\n",
      "\n",
      "Future work will analyze the developed framework in various manufacturing scenarios. We will involve multiple users without prior assembly knowledge, allowing free in- teraction to complete tasks, evaluate the clarity of the LLM- generated message and compare the system to other methods. We will assess the importance of the initialization protocol\n",
      "\n",
      "by comparing different options and testing the framework on other material handling and manufacturing tasks.\n",
      "\n",
      "V. CONCLUSION AND FUTURE WORK\n",
      "\n",
      "Advancements in LLM are applied to human-robot col- laborative assembly to execute actions and collaborate based on environmental data. Incorporating LLM allows robots to better understand human operators, resolve errors, and improve execution with environmental feedback. Based on these insights, we integrated LLM into our assembly frame- work for dynamic responses to task variations.\n",
      "\n",
      "This research addresses key challenges in human-robot collaborative assembly, including developing communication systems that require minimal robotics training (C1), enhanc- ing adaptability to manage changes and errors (C2), and in- tegrating advanced technologies with a human-centric design to improve usability (C3). We validated our framework using the cable shark device assembly process, demonstrating its ability to facilitate intuitive human-robot communication via voice commands with language variations. We dynamically adapt to task variations and errors by integrating LLM, sensors, and task control mechanisms, demonstrating its ability to maintain productivity and ensure a continuous workflow.\n",
      "\n",
      "Our next steps will test the framework under real indus- trial conditions, including operator variations and different manufacturing environments (e.g., noise, dust, brightness). Additionally, future work includes extending the LLM-based framework to increase adaptability by feeding the LLM with diverse data on robotic tasks and sensor information. This will enhance the robot’s task flexibility, safety, and ability to handle unexpected errors. We also plan to in- corporate multiple modalities, such as haptic and gestures to improve human-robot interaction. By integrating LLM to this multimodal strategy, we aim to improve communication efficiency, task adaptability, and intuitive interaction in the manufacturing environment.\n",
      "\n",
      "ACKNOWLEDGMENT\n",
      "\n",
      "We thank Dana Smith from DMI Companies, Inc. for providing the case study and offering valuable feedback on the project.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "[1] Paryanto, M. Brossog, M. Bornschlegl, and J. Franke, “Reducing the Energy Consumption of Industrial Robots in Manufacturing Systems,” The International Journal of Advanced Manufacturing Technology, vol. 78, pp. 1315–1328, 2015.\n",
      "\n",
      "[2] L. Wang, S. Liu, H. Liu, and X. V. Wang, “Overview of Human-Robot\n",
      "\n",
      "Collaboration in Manufacturing,” pp. 15–58, 2020.\n",
      "\n",
      "[3] C. S. Franklin, E. G. Dominguez, J. D. Fryman, and M. L. Lewandowski, “Collaborative Robotics: New Era of Human–Robot Cooperation in the Workplace,” Journal of Safety Research, vol. 74, pp. 153–160, 2020.\n",
      "\n",
      "[4] A. M. Zanchettin, N. M. Ceriani, P. Rocco, H. Ding, and B. Matthias, “Safety in Human-Robot Collaborative Manufacturing Environments: Metrics and Control,” IEEE Transactions on Automation Science and Engineering, vol. 13, no. 2, pp. 882–893, 2015.\n",
      "\n",
      "[5] M. Pearce, B. Mutlu, J. Shah, and R. Radwin, “Optimizing Makespan and Ergonomics in Integrating Collaborative Robots into Manufac- turing Processes,” IEEE Transactions on Automation Science and Engineering, vol. 15, no. 4, pp. 1772–1784, 2018.\n",
      "\n",
      "[6] M. Javaid, A. Haleem, R. P. Singh, and R. Suman, “Substantial Capabilities of Robotics in Enhancing Industry 4.0 Implementation,” Cognitive Robotics, vol. 1, pp. 58–75, 2021.\n",
      "\n",
      "[7] K. Bogner, U. Pferschy, R. Unterberger, and H. Zeiner, “Optimised Scheduling in Human–Robot Collaboration–A Use Case in the As- sembly of Printed Circuit Boards,” International Journal of Production Research, vol. 56, no. 16, pp. 5522–5540, 2018.\n",
      "\n",
      "[8] U. Körner, K. Müller-Thur, T. Lunau, N. Dragano, P. Angerer, and A. Buchner, “Perceived Stress in Human–Machine Interaction in Mod- ern Manufacturing Environments—Results of a Qualitative Interview Study,” Stress and Health, vol. 35, no. 2, pp. 187–199, 2019.\n",
      "\n",
      "[9] E. Matheson, R. Minto, E. G. Zampieri, M. Faccio, and G. Rosati, “Human–Robot Collaboration in Manufacturing Applications: A Re- view,” Robotics, vol. 8, no. 4, p. 100, 2019.\n",
      "\n",
      "[10] L. Floridi and M. Chiriatti, “GPT-3: Its Nature, Scope, Limits, and\n",
      "\n",
      "Consequences,” Minds and Machines, vol. 30, pp. 681–694, 2020.\n",
      "\n",
      "[11] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4 Technical Report,” arXiv preprint arXiv:2303.08774, 2023.\n",
      "\n",
      "[12] J. de Gea Fernández, D. Mronga, M. Günther, T. Knobloch, M. Wirkus, M. Schröer, M. Trampler, S. Stiene, E. Kirchner, V. Bargsten et al., “Multimodal Sensor-Based Whole-Body Control for Human–Robot Collaboration in Industrial Settings,” Robotics and Autonomous Sys- tems, vol. 94, pp. 102–119, 2017.\n",
      "\n",
      "[13] D. Wei, L. Chen, L. Zhao, H. Zhou, and B. Huang, “A Vision- Based Measure of Environmental Effects on Inferring Human Intention During Human Robot Interaction,” IEEE Sensors Journal, vol. 22, no. 5, pp. 4246–4256, 2021.\n",
      "\n",
      "[14] H. Liu, T. Fang, T. Zhou, and L. Wang, “Towards Robust Human- Robot Collaborative Manufacturing: Multimodal Fusion,” IEEE Ac- cess, vol. 6, pp. 74 762–74 771, 2018.\n",
      "\n",
      "[15] W. Wang, R. Li, Y. Chen, and Y. Jia, “Human intention pre- diction in human-robot collaborative tasks,” in Companion of the 2018 ACM/IEEE international conference on human-robot interaction, 2018, pp. 279–280.\n",
      "\n",
      "[16] W. Wang, R. Li, Y. Chen, Z. M. Diekel, and Y. Jia, “Facilitating human–robot collaborative tasks by teaching-learning-collaboration from human demonstrations,” IEEE Transactions on Automation Sci- ence and Engineering, vol. 16, no. 2, pp. 640–653, 2018.\n",
      "\n",
      "[17] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embod- ied Agents,” 2022.\n",
      "\n",
      "[18] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, “ProgPrompt: Generating Situated Robot Task Plans Using Large Language Models,” 2022.\n",
      "\n",
      "[19] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg, “Text2Motion: From Natural Language Instructions to Feasible Plans,” Autonomous Robots, vol. 47, no. 8, p. 1345–1365, Nov. 2023.\n",
      "\n",
      "[20] O. Mees, J. Borja-Diaz, and W. Burgard, “Grounding Language with\n",
      "\n",
      "Visual Affordances Over Unstructured Data,” 2023.\n",
      "\n",
      "[21] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, and C. Gan, “Building Cooperative Embodied Agents Modularly with Large Language Models,” 2024.\n",
      "\n",
      "[22] A. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu, L. Takayama, F. Xia, J. Varley, Z. Xu, D. Sadigh, A. Zeng, and A. Majumdar, “Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners,” 2023.\n",
      "\n",
      "[23] G. Zheng, I. Kovalenko, K. Barton, and D. Tilbury, “Integrating Human Operators into Agent-Based Manufacturing Systems: A Table- Top Demonstration,” Procedia manufacturing, vol. 17, pp. 326–333, 2018.\n",
      "\n",
      "[24] UFACTORY, Nov 2023. [Online]. Available: https://www.ufactory.cc/ [25] OpenAI, “Speech to Text.” [Online]. Available: https://platform.\n",
      "\n",
      "openai.com/docs/guides/speech-to-text\n",
      "\n",
      "[26] ——, “Text to Speech.” [Online]. Available: https://platform.openai.\n",
      "\n",
      "com/docs/guides/text-to-speech\n",
      "\n",
      "[27] ——, “Function Calling.” [Online]. Available: https://platform.openai.\n",
      "\n",
      "com/docs/guides/function-calling\n",
      "\n",
      "[28] G. Jocher, “YOLOv5 by Ultralytics,” May 2020. [Online]. Available:\n",
      "\n",
      "https://github.com/ultralytics/yolov5\n",
      "\n",
      "[29] L. Xia, C. Li, C. Zhang, S. Liu, and P. Zheng, “Leveraging error- assisted fine-tuning large language models for manufacturing excel- lence,” Robotics and Computer-Integrated Manufacturing, vol. 88, p. 102728, 2024.\n",
      "\n",
      "[30] J. Lim, L. Pfeiffer, F. Ocker, B. Vogel-Heuser, and I. Kovalenko, “Ontology-Based Feedback to Improve Runtime Control for Multi- Agent Manufacturing Systems,” pp. 1–7, 2023.\n",
      "[Document(page_content='Enhancing Human-Robot Collaborative Assembly in Manufacturing Systems Using Large Language Models\\n\\nJonghan Lim1, Sujani Patel2, Alex Evans3, John Pimley1, Yifei Li2, and Ilya Kovalenko1,2', metadata={'source': 'paper1.pdf'}), Document(page_content='Abstract— The development of human-robot collaboration has the ability to improve manufacturing system performance by leveraging the unique strengths of both humans and robots. On the shop floor, human operators contribute with their adapt- ability and flexibility in dynamic situations, while robots provide precision and the ability to perform repetitive tasks. However, the communication gap between human operators and robots limits the collaboration and coordination of human-robot teams in manufacturing systems. Our research presents a human- robot collaborative assembly framework that utilizes a large language model for enhancing communication in manufac- turing environments. The framework facilitates human-robot communication by integrating voice commands through natural language for task management. A case study for an assembly task demonstrates the framework’s ability to process natural language inputs and address real-time assembly challenges, emphasizing adaptability to', metadata={'source': 'paper1.pdf'}), Document(page_content='for task management. A case study for an assembly task demonstrates the framework’s ability to process natural language inputs and address real-time assembly challenges, emphasizing adaptability to language variation and efficiency in error resolution. The results suggest that large language models have the potential to improve human-robot interaction for collaborative manufacturing assembly applications.', metadata={'source': 'paper1.pdf'}), Document(page_content='I. INTRODUCTION Advances in robotics technology have significantly en- hanced manufacturing efficiency, resulting in cost reductions and increased productivity [1], [2]. While robots can execute rapid, accurate, and repetitive tasks that demand heavy-duty efforts in manufacturing settings, they lack the capability for adaptation and versatility of human operators [2]. There- fore, the importance of Human-Robot Collaboration (HRC) is growing as humans and robots complement each other skills and capabilities. HRC refers to the interaction and cooperation between human operators and robotic systems within shared workspace [3]. Prior works have used HRC frameworks to improve ergonomics in manufacturing settings and safe human-robot interaction without compromising pro- ductivity [4], [5]. Transitioning from tasks involving large components, such as loading, placing, and unloading, to the complex assembly of smaller components such as printed circuit boards, the collaboration between human', metadata={'source': 'paper1.pdf'}), Document(page_content='from tasks involving large components, such as loading, placing, and unloading, to the complex assembly of smaller components such as printed circuit boards, the collaboration between human operators and robots significantly enhances both the efficiency and safety of production lines [6], [7].', metadata={'source': 'paper1.pdf'}), Document(page_content='Further advancing HRC in manufacturing systems by improving the interaction of human operators and robots presents significant challenges. Specifically, interaction with robots induces psychological stress and tension among opera- tors due to language barrier [8]. In contemporary manufactur- ing systems, human operators require extensive pre-service\\n\\nThe authors [1] are with the Department of Industrial and Manufacturing, [2] are with the Department of Mechanical Engineering, and [3] are with the Department of Computer Science and Engineering, at the Pennsylvania State University, State College, USA (e-mail: {jxl567; spp22; ade5221; jap6581; ybl5717; iqk5135}@psu.edu).', metadata={'source': 'paper1.pdf'}), Document(page_content='training and complex code development to ensure accurate and safe production with the robots [9]. These difficulties highlight the need to develop human-robot communication systems that enable interaction between humans and robots without extensive robotics training (C1). Another challenge involves the need for greater flexibility and adaptability in human-robot interaction. As the number of interactions between humans and robots on the shop floor increases, there is a higher chance of encountering unexpected changes and errors. Thus, HRC needs to be more adaptable to changes and errors during the manufacturing assembly process (C2). Ad- ditionally, human-robot collaborative assembly applications must integrate advanced technologies with a human-centric design to improve communication and usability (C3).', metadata={'source': 'paper1.pdf'}), Document(page_content='Large Language Models (LLMs) have recently been in- troduced in the AI community to enhance natural language understanding and generation capabilities. These can be extended to improve human-robot interaction in the man- ufacturing facility. Models such as OpenAI’s GPT-3 [10] and GPT-4 [11], have shown proficiency in processing, understanding, and communicating in natural language. The integration of LLMs facilitates natural language communi- cation between humans and robots. Using voice interaction for this communication enhances collaboration and operator safety in dynamic workspaces. The main contributions of this work are: (1) the use of LLMs for interpreting natural language to allow operators to coordinate with robotic arms, (2) a framework for system integration of voice commands, robotic arms, and vision systems to facilitate HRC in an assembly task, enhancing operational flexibility, and (3) an adaptation to task errors and obstacles through human-robot communication in a', metadata={'source': 'paper1.pdf'}), Document(page_content='robotic arms, and vision systems to facilitate HRC in an assembly task, enhancing operational flexibility, and (3) an adaptation to task errors and obstacles through human-robot communication in a manufacturing environment.', metadata={'source': 'paper1.pdf'}), Document(page_content='The rest of the manuscript is organized as follows. Sec- tion II reviews related work and identifies gaps in HRC for manufacturing assembly. Section III describes the proposed integrates LLMs with voice commands, framework that robotic arm, and sensors. Section IV showcases a case study and evaluates the framework’s performance. Finally, Section V provides conclusions and future work.\\n\\nII. BACKGROUND\\n\\nA. Human-Robot Collaboration in Manufacturing\\n\\nRecently, a wide range of methods has been developed in the field of HRC to improve the safety and efficiency of interactions between humans and robots. For instance, Fer- nandez et al. [12] developed a dual-arm robotic system with multisensor capabilities for safe and efficient collaboration, integrating gesture recognition. Additionally, Wei et al. [13]\\n\\ngather information on human operators and the environment using RGB-D videos and predict human intentions based on deep-learning methods.', metadata={'source': 'paper1.pdf'}), Document(page_content='gather information on human operators and the environment using RGB-D videos and predict human intentions based on deep-learning methods.\\n\\nSome approaches have utilized multi-modal strategies, including natural language, to improve communication for HRC in manufacturing. For example, Liu et al. [14] focus on integrating various modalities, including speech commands, hand motions, and body motions to improve HRC. While it addresses speech command recognition using deep learning models, the interaction is limited to basic speech command recognition and does not focus on context-aware communica- tion. Additionally, Wang et al. [15], [16] employ a teaching- learning model that includes natural language instructions to predict human intentions and facilitate collaboration. While this model uses natural language for multimodal processing, it does not emphasize utilizing language variations in inter- action.', metadata={'source': 'paper1.pdf'}), Document(page_content='Previous research have introduced methods using envi- ronmental data to enhance safety and efficiency and natural language to improve HRC in manufacturing. However, there is limited research on human-robot collaborative assembly that effectively integrates natural language capabilities for context-aware communication and handling language vari- ations. We aim to integrate an LLM-based approach to improve communication between humans and robots. Our approach is an initial step towards enhancing human-robot interaction by combining existing technologies, such as com- puter vision and LLM, to leverage human flexibility and robot precision in manufacturing.\\n\\nB. Large Language Models for Robots', metadata={'source': 'paper1.pdf'}), Document(page_content='Recent research has explored integrating LLMs in robotic applications to enhance human-robot interaction and robot functionalities [17], [18], [19], [20], [21], [22]. This includes converting high-level human instructions into tasks robots can execute. Huang et al. [17] used LLMs, such as GPT-3 and CODEX, to convert high-level human instructions into executable actions for robots. Similarly, Singh et al. [18] developed a method called ProgPrompt, which generates Python code for task planning that accounts for environmen- tal conditions and pre-programmed actions. Furthermore, Lin et al. [19] developed a framework for taking natural language instructions and then developing task and motion plans. Other works in robotics have utilized LLMs to maximize the robot’s capability to comprehend and interact with its environment to resolve errors. Mees et al [20] combined natural language input with visual information to enhance task completion ability. Zhang et al. [21] utilizes LLM to address', metadata={'source': 'paper1.pdf'}), Document(page_content='interact with its environment to resolve errors. Mees et al [20] combined natural language input with visual information to enhance task completion ability. Zhang et al. [21] utilizes LLM to address multi-agent cooperation challenges, resolving issues related to decentralized control, raw sensory data process- ing, and efficient task execution in embodied environments. KnowNo [22] improved robot autonomy and efficiency by guiding robots in seeking human help when faced with uncertainty.', metadata={'source': 'paper1.pdf'}), Document(page_content='These studies show how integrating LLMs into robotic applications improves human-robot interaction and robot capabilities. However, existing studies have not addressed\\n\\ncommon challenges found in the manufacturing environment, such as matching natural language to structured task se- quences and managing the timing of commands (e.g., iden- tifying when to request operator assistance in the process). We aim to bridge the gap by improving the integration of natural language for human-robot interaction for an assembly process through an LLM-based framework.\\n\\nIII. FRAMEWORK\\n\\nThe proposed framework, presented in Figure 1, is devel- oped for a human-robot collaborative assembly within the manufacturing environment. The framework is designed to facilitate interaction between a human operator and a robot for an assembly process.', metadata={'source': 'paper1.pdf'}), Document(page_content='In this framework, a human can initialize and provide a set of tasks T = {t1, t2, . . . , tn} as the objectives to be achieved by the robot. While T is predefined, the system can be extended to integrate new tasks based on human input. The robot’s capabilities C = {c1, c2, . . . , cl} represent the specific actions programmed to perform within an assembly process. If the task matches the robot’s capabilities, the robot should execute the specified task ti. The task, ti, is broken down into subtasks (ti1, ti2, . . . , tik) that outline the sequential steps for completion. The framework has to track the last completed subtask, denoted as tic, where 1 ≤ c ≤ k indicates the completed process within ti1, ti2, . . . , tik. When an error is resolved by a human operator, the robot resumes from tic+1, ensuring continuous task progression. To enable this human-robot collaborative assembly process, this framework is structured into two primary layers: the physical layer and the virtual layer,', metadata={'source': 'paper1.pdf'}), Document(page_content='tic+1, ensuring continuous task progression. To enable this human-robot collaborative assembly process, this framework is structured into two primary layers: the physical layer and the virtual layer, bridging the gap between human inputs and robot actions.', metadata={'source': 'paper1.pdf'}), Document(page_content='A. Physical Layer\\n\\ninteraction within a shared space, based on data from the virtual layer. The physical consists of three primary components: human commands, robot actions, and sensor data. Human com- mands involve the operator providing voice instructions to control robot actions. Robot actions are based on a predefined set of T . Sensor data is utilized to monitor environmental conditions. This data ensures the action adapts to changes in the workspace (e.g. position, orientation, or availability of parts).\\n\\nThe physical\\n\\nlayer\\n\\nfacilitates human-robot', metadata={'source': 'paper1.pdf'}), Document(page_content='The physical\\n\\nlayer\\n\\nfacilitates human-robot\\n\\nWhen a predefined event or error ei ∈ E is detected, the robot alerts the human operator using a communication protocol. The LLM module converts ei into a natural lan- guage message Mei(ti), delivered via text-to-speech tech- nology. Different instances of ei can lead to varied message Mei(ti) depending on the task ti error. After receiving and understanding Mei(ti), the human operator resolves the corresponding ei and commands the robot to resume ti.\\n\\nB. Virtual Layer\\n\\nThe virtual layer serves to facilitate communication. This layer stores the system’s functionality to enable interaction between human commands and robot actions. The virtual\\n\\nFig. 1: Human-Robot Collaborative Assembly Framework Using LLM\\n\\nlayer of the system consists of two main agents: a human agent and a robot agent.', metadata={'source': 'paper1.pdf'}), Document(page_content='1) Human Agent: We have integrated a human agent, often referred to as a human command interpreter, to enhance the human-robot interaction based on the approach by Zhang is to process et al. [23]. The goal of the human agent the human commands in voice instructions into a text that the robot agent can understand. This agent ensures that the human intentions are transferred to the robot without manual programming. The human agent contains an Au- tomatic Speech Recognition (ASR) module responsible for processing the human command voice data, which converts this data into text that the robot agent can process. The communication module in the human agent serves as a bridge, transmitting commands and information to the robot. 2) Robot Agent: The robot agent interprets and executes the set of manufacturing tasks T based on the voice com- mands received from the human operator. This process is facilitated by various functional modules:', metadata={'source': 'paper1.pdf'}), Document(page_content='Initialization Module: The initialization of the robot agent, provides basic operational guidelines and task execution protocols. This process involves programming the robot with instructions to perform tasks T within its capabilities C and to communicate errors or request assistance from human operators when necessary. The following initial prompt is given to the robot agent: collaborative assembly system designed to assist\\n\\nin tasks and respond to commands. Upon receiving\\n\\na request within your capability range, execute\\n\\nthe service. In the event of encountering errors,\\n\\nrequest assistance from a human operator for error\\n\\ncorrection, providing clear and understandable', metadata={'source': 'paper1.pdf'}), Document(page_content='a request within your capability range, execute\\n\\nthe service. In the event of encountering errors,\\n\\nrequest assistance from a human operator for error\\n\\ncorrection, providing clear and understandable\\n\\nThis initialization prompt defines the robot agent’s role in responding to commands. It also establishes a pro- tocol for seeking assistance and communicating errors to human operators for task completion. Following the initial setup of the robot agent, specific capabilities C are defined, such as specific assembly tasks.', metadata={'source': 'paper1.pdf'}), Document(page_content='LLM Module: An LLM interprets human commands into task ti ∈ T using initialization prompts and detailed functional information. Human commands and func- tional capabilities C including the detailed functional information of C are put into the LLM. The LLM then analyzes the language of the human commands and understands the context based on the provided information, such as the last subtask completed and the error encountered, enabling it to detect the next task automatically. This process allows the system to match the command with the predefined functional capabilities C, ensuring the intended task is executed. The LLM module also communicates any detected issues from the task control module into understandable natural language for the human operators, facilitating HRC for error resolution. While static error messages do not cap- ture the context of an error event, processing these with LLM adapts to the situation, providing more actionable information to human operators. This', metadata={'source': 'paper1.pdf'}), Document(page_content='While static error messages do not cap- ture the context of an error event, processing these with LLM adapts to the situation, providing more actionable information to human operators. This approach helps human operators understand the next steps based on the robot’s guidance.', metadata={'source': 'paper1.pdf'}), Document(page_content='Sensor Module: The sensor module processes data from the physical layer such as position, orientation, and part availability. This information is utilized for adjust- ing robot actions required for executing manufacturing tasks T . For example, the module provides component positions and orientations for the task control module. This enables accurate robotic adjustments for assembly\\n\\nexplanations.\"\\n\\nFig. 2: Sequence Diagram for Human-Robot Collaborative Assembly in Manufacturing Systems', metadata={'source': 'paper1.pdf'}), Document(page_content='explanations.\"\\n\\nFig. 2: Sequence Diagram for Human-Robot Collaborative Assembly in Manufacturing Systems\\n\\nor initiates error notifications for missing components, ensuring effective task execution and error management. • Task Control Module: The task control module directs robot actions to fulfill task ti within the set T , based on the functional capabilities C. The task control mod- ule adjusts robot actions to environmental conditions this obtained from the sensor module. Additionally, module plays a role in managing errors by verifying sensor data and communicating detected issues through the LLM module to facilitate resolution by the human operator.\\n\\nC. Human-Robot Collaborative Assembly Workflow\\n\\nThe overall workflow is shown in a sequence diagram in Figure 2, illustrating the human-robot collaborative assembly process. The diagram depicts how voice commands from the human operator are processed by the LLM module to guide robot actions.', metadata={'source': 'paper1.pdf'}), Document(page_content='The process starts with the operator giving a voice com- mand, converted by the LLM module into a discrete set of tasks T for the robot. The robot then requests sensor data to execute ti. If the data is valid, the robot proceeds to execute the assigned ti. The sensor module determines data validity by comparing detected parameters to predefined criteria. Successful execution results in a completion message Mc(ti) to the operator via the LLM Module.\\n\\nthe robot generates an error message Mei (ti) via the LLM module, aiming to inform the human operator of the specific error and its occurrence within subtask tic+1 for efficient resolution. Following the error identification and correction by the human operator, a new command by the human operator is issued to the robot. The robot then resumes task execution at ti, starting from the interrupted subtask tic+1, based on the new sensor data. This procedure is repeated until ti is completed.\\n\\nIf the data is invalid, or ti has any errors,', metadata={'source': 'paper1.pdf'}), Document(page_content='If the data is invalid, or ti has any errors,\\n\\nIV. CASE STUDY\\n\\nThe proposed framework was tested in a manufacturing assembly manufacturing cell. The goal of the manufacturing\\n\\nFig. 3: Cable Shark Assembly\\n\\ncell is to assemble a cable shark product. The cable shark assembly contains (1) a housing, (2) a wedge, (3) a spring, (4) and an end cap. The exploded cable shark assembly and the final assembled product are shown in Figure 3. This assembly process task t1 include four sequential subtasks: housing assembly t11, wedge assembly t12, spring assembly t13, and end cap assembly t14.', metadata={'source': 'paper1.pdf'}), Document(page_content='The physical setup uses a Six Degrees of Freedom (6DOF) UFactory xArm with a UFactory 2-finger gripper as the end effector [24]. The base of the robotic arm remains fixed at the center, accompanied by a mat on one side, while the parts are assembled on the adjacent side. Two Intel RealSense Depth Camera D435 cameras were used as visual sensors in the surrounding area, one focused on the mat area for pre-assembly component placement, and the other camera on the assembling process. The conversion between camera and robot arm coordinates is accomplished through the coordinate transformation. We used a fixed z-axis value for the robotic arm to pick up camera-identified parts. In future work, we aim to make this z-axis value adaptable.\\n\\nA. LLM and ASR Module', metadata={'source': 'paper1.pdf'}), Document(page_content='A. LLM and ASR Module\\n\\nThis section outlines how LLM and the ASR module, were implemented within the system. The communication aspect within the system is enabled by OpenAI’s speech-to- text and text-to-speech models. The OpenAI’s transcription model ‘whisper1’ [25] is implemented to transform speech- to-text, ensuring that human voice instructions are accurately captured. To enable the communication protocol to provide a verbal response from the robot, OpenAI’s text-to-speech model ‘tts-1’ [26] is utilized to generate audio responses. This approach enhances and allows for a clear exchange of information between the human and the robot.', metadata={'source': 'paper1.pdf'}), Document(page_content='The LLM module leverages OpenAI’s pre-trained GPT- 4.0 [11]. The LLM module converts instructions from the human operator into tasks T executable by the robot, uti- lizing its capabilities C through OpenAI’s function calling feature [27]. This module ensures that the robot actions match the human operator commands, allowing for adaptive and responsive task management within the manufacturing environment.\\n\\nB. Sensor Module: Vision System\\n\\nWe incorporated a vision system as our sensor module. During the collaborative assembly process, the vision sys- tem provides feedback on environmental data to the task control module. This enables precise object detection and identification of object orientation, facilitating task execution\\n\\nFig. 4: Feature Extraction Method with the Vision System', metadata={'source': 'paper1.pdf'}), Document(page_content='and error management. To enable object detection, Yolov5, a computer vision model, is utilized [28]. Custom Yolov5 models are trained using a dataset of images of individual parts (i.e. housing, wedge, spring, end cap), with manual bounding box annotations around each part in the images. The vision system these annotations to identify the parts, as shown in Figure 4. If a part is detected, the coordinates of the top-left and bottom-right corners of the bounding box are retrievable. Once these coordinates are obtained, the x and y coordinates of these points are utilized to determine the object’s midpoint. This midpoint serves as the target point for the robotic arm to pick up the part. Following the successful identification, the valid sensor data is transferred to the task control module for guiding robot action. For inaccurately detected parts, the sensor module transfers invalid data to the task control module to notify the human operator via LLM.', metadata={'source': 'paper1.pdf'}), Document(page_content='C. Task Control Module: Assembly Task', metadata={'source': 'paper1.pdf'}), Document(page_content='The Task Control Module executes tasks from human commands interpreted by the LLM and manages errors in a human-robot collaborative assembly system. The task control module verifies sensor data from the vision system. For instance, if the data is valid, the task control module proceeds; if not, the task control module sends error details (e.g. missing component) to the LLM module to convert the information into natural language. This process activates the communication protocol, allowing the human operator to recognize and address the error. To enable the assembly process, the task control module integrates various function- alities, including coordinate transformation and position and rotation control, based on the sensor data obtained from the vision system. These functions enable the execution of tasks interpreted by the LLM. The orientation of detected objects is determined from the bounding box dimensions, assuming the object’s angle is perpendicular or parallel to the x-axis of', metadata={'source': 'paper1.pdf'}), Document(page_content='of tasks interpreted by the LLM. The orientation of detected objects is determined from the bounding box dimensions, assuming the object’s angle is perpendicular or parallel to the x-axis of the camera vision. The cable shark assembly process is shown in Figure 5.', metadata={'source': 'paper1.pdf'}), Document(page_content='D. Case Study Results\\n\\nThe proposed framework was integrated into the assembly system to study the effect of integrating LLMs with a knowledgeable operator1. The operator knew the assembly process and was able to converse. Three scenarios were used to evaluate the proposed framework: Scenario 1: The system detects an overlap of the housing components and requests human intervention.\\n\\n1See supplementary video file for a demonstration of the human-robot\\n\\ncollaborative assembly.\\n\\nFig. 5: Cable Shark Assembly Process\\n\\nScenario 2: If a wedge component is incorrectly assembled, the robot halts and requests human correction. Scenario 3: A missing spring component is detected and the robot requests the human operator to place the spring component.', metadata={'source': 'paper1.pdf'}), Document(page_content='Based on these scenarios, the evaluation assessed the sys- tem’s proficiency in interpreting and performing tasks from human commands, considering the linguistic and human diversity. Variations in phrasing and terminology across dif- ferent commands were cataloged, as presented in Table I, to validate the system’s adaptability to diverse linguistic inputs. Instructions were classified into three distinct categories: specific, moderately specific, and least specific. Five unique variations of each instruction type were provided across tasks. To ensure reliability, each language three distinct variation scenario presented in Table I is conducted with three repetitions.', metadata={'source': 'paper1.pdf'}), Document(page_content='The results highlight the effectiveness of integrating LLM into the human-robot collaborative assembly framework. Figure 6 showcases the human-robot communication using the vision system, outlined in Section IV-B, to ensure task completion. In scenario 1, the robot detects overlapping components which signal to human operators for help. Upon resolution, the human operator prompts task continuation. Similarly in scenario 2, the robot identifies a misassembled wedge, notifying the human operator, and then the human commands the robot to continue after manually assembling\\n\\nTABLE I: Language Variations for Task Instructions\\n\\nScenario 1\\n\\nScenario 1: Component Overlap\\n\\nScenario 2: Incorrectly Assembled Part\\n\\nScenario 3: Missing Component\\n\\nInstruction Type Specific Moderately Specific Least Specific Specific Moderately Specific Least Specific Specific Moderately Specific Least Specific', metadata={'source': 'paper1.pdf'}), Document(page_content='Scenario 3: Missing Component\\n\\nInstruction Type Specific Moderately Specific Least Specific Specific Moderately Specific Least Specific Specific Moderately Specific Least Specific\\n\\nInstruction [\"Overlap resolved. Proceed with the task.\"], [\"Problem is...\"] [\"I’ve placed the components correctly.\"], [\"I’ve sorted out the...\"] [\"Fixed.\"], [\"Done.\"], [\"Completed.\"], [\"Handled.\"], [\"Adjusted.\"] [\"Correction is made. Resume the task.\"], [\"The wedge is...\"] [\"I’ve fixed the issue with the wedge.\"], [\"I’ve placed the wedge...\"] [\"Fixed.\"], [\"Done.\"], [\"Addressed.\"], [\"All set.\"], [\"Under control.\"] [\"I’ve placed the spring component. Please proceed.\"].[\"Spring...\"] [\"I’ve fixed the issue with the spring.\"], [\"The spring component...\"] [\"Fixed.\"], [\"Done.\"], [\"Managed.\"], [\"Handled\"], [\"Settled.\"]\\n\\nTABLE II: Success Rates for Language Variations\\n\\nE. Case Study Discussion and Limitations\\n\\nScenario\\n\\nScenario 1: Component Overlap\\n\\nScenario 2: Incorrectly Assembled Part', metadata={'source': 'paper1.pdf'}), Document(page_content='TABLE II: Success Rates for Language Variations\\n\\nE. Case Study Discussion and Limitations\\n\\nScenario\\n\\nScenario 1: Component Overlap\\n\\nScenario 2: Incorrectly Assembled Part\\n\\nScenario 3: Missing Component\\n\\nInstruction Category Specific Moderately Specific Least Specific Specific Moderately Specific Least Specific Specific Moderately Specific Least Specific\\n\\nSuccess Rate 100% 73% 27% 93% 87% 53% 100% 67% 27%', metadata={'source': 'paper1.pdf'}), Document(page_content='The case study evaluated the system’s ability to facilitate collaboration between humans and robots and showcases how the integration of LLMs can lead to efficient and flexible manufacturing processes. The results show that as instruc- tions become less specific, robot performance significantly decreases, indicating the need for well-defined commands. For example, the ambiguous command \"Correction is made. Resume the operations.\" failed due to lack of context and explicit task reference. These results highlight the limita- tions of the proposed framework and suggest an area for improvement. One such improvement involves introducing feedback mechanisms that allow the system to ask for clari- fication when instructions are unclear, refining the interaction between humans and robots. Another solution is to fine- tune the LLM [29] or integrate a knowledge base [30] with specific manufacturing processes to improve its handling of instructions and their correct responses. Commands contain-', metadata={'source': 'paper1.pdf'}), Document(page_content='solution is to fine- tune the LLM [29] or integrate a knowledge base [30] with specific manufacturing processes to improve its handling of instructions and their correct responses. Commands contain- ing keywords like ’fixed’ or ’addressed’ had higher success rates, emphasizing the importance of initialization protocol. The protocol informs the robot that the human’s role is to correct errors to resume the assembly task. The case study found that a clear initialization process is crucial for the robot to understand its role and functions within the system.', metadata={'source': 'paper1.pdf'}), Document(page_content='Fig. 6: Case Study Communication Results for Each Scenario\\n\\nthe wedge. For Scenario 3, the robot flags a missing spring component, requesting human intervention, and the human instructs the robot to resume the assembly once the issue is fixed.\\n\\nThe evaluation assessed the system’s capability to un- derstand and execute commands with varied language ex- pressions. Success rate refers to the percentage of the robot correctly understanding and executing the given instruction without requiring additional clarification. For tasks with specific instructions, success rates were high, averaging approximately 98%. The rates decreased to 76% with mod- erately specific instructions and dropped further to 36% with least specific instructions. The success rates are detailed in Table II. This data suggests a positive correlation between instruction type and task execution success.', metadata={'source': 'paper1.pdf'}), Document(page_content='There were also limitations in the case study and eval- uation of the proposed framework. Specifically, we only evaluated a limited range of commands from the operator, rather than assessing the overall system capabilities. These commands were related to predefined assembly scenarios and did not include other interactions, such as human in- terruptions and task-irrelevant questions from the human. While LLM-generated error messages were understandable for knowledgeable operators, there is a risk of low-quality messages for less experienced operators or in varied contexts. The case study also did not study the variability in the operator’s knowledge, as the framework assumed operators would provide relevant instructions to the assembly task. Furthermore, the operator was not allowed to change the defined tasks, e.g., variations in positioning or task order.', metadata={'source': 'paper1.pdf'}), Document(page_content='Future work will analyze the developed framework in various manufacturing scenarios. We will involve multiple users without prior assembly knowledge, allowing free in- teraction to complete tasks, evaluate the clarity of the LLM- generated message and compare the system to other methods. We will assess the importance of the initialization protocol\\n\\nby comparing different options and testing the framework on other material handling and manufacturing tasks.\\n\\nV. CONCLUSION AND FUTURE WORK\\n\\nAdvancements in LLM are applied to human-robot col- laborative assembly to execute actions and collaborate based on environmental data. Incorporating LLM allows robots to better understand human operators, resolve errors, and improve execution with environmental feedback. Based on these insights, we integrated LLM into our assembly frame- work for dynamic responses to task variations.', metadata={'source': 'paper1.pdf'}), Document(page_content='This research addresses key challenges in human-robot collaborative assembly, including developing communication systems that require minimal robotics training (C1), enhanc- ing adaptability to manage changes and errors (C2), and in- tegrating advanced technologies with a human-centric design to improve usability (C3). We validated our framework using the cable shark device assembly process, demonstrating its ability to facilitate intuitive human-robot communication via voice commands with language variations. We dynamically adapt to task variations and errors by integrating LLM, sensors, and task control mechanisms, demonstrating its ability to maintain productivity and ensure a continuous workflow.', metadata={'source': 'paper1.pdf'}), Document(page_content='Our next steps will test the framework under real indus- trial conditions, including operator variations and different manufacturing environments (e.g., noise, dust, brightness). Additionally, future work includes extending the LLM-based framework to increase adaptability by feeding the LLM with diverse data on robotic tasks and sensor information. This will enhance the robot’s task flexibility, safety, and ability to handle unexpected errors. We also plan to in- corporate multiple modalities, such as haptic and gestures to improve human-robot interaction. By integrating LLM to this multimodal strategy, we aim to improve communication efficiency, task adaptability, and intuitive interaction in the manufacturing environment.\\n\\nACKNOWLEDGMENT\\n\\nWe thank Dana Smith from DMI Companies, Inc. for providing the case study and offering valuable feedback on the project.\\n\\nREFERENCES', metadata={'source': 'paper1.pdf'}), Document(page_content='ACKNOWLEDGMENT\\n\\nWe thank Dana Smith from DMI Companies, Inc. for providing the case study and offering valuable feedback on the project.\\n\\nREFERENCES\\n\\n[1] Paryanto, M. Brossog, M. Bornschlegl, and J. Franke, “Reducing the Energy Consumption of Industrial Robots in Manufacturing Systems,” The International Journal of Advanced Manufacturing Technology, vol. 78, pp. 1315–1328, 2015.\\n\\n[2] L. Wang, S. Liu, H. Liu, and X. V. Wang, “Overview of Human-Robot\\n\\nCollaboration in Manufacturing,” pp. 15–58, 2020.\\n\\n[3] C. S. Franklin, E. G. Dominguez, J. D. Fryman, and M. L. Lewandowski, “Collaborative Robotics: New Era of Human–Robot Cooperation in the Workplace,” Journal of Safety Research, vol. 74, pp. 153–160, 2020.\\n\\n[4] A. M. Zanchettin, N. M. Ceriani, P. Rocco, H. Ding, and B. Matthias, “Safety in Human-Robot Collaborative Manufacturing Environments: Metrics and Control,” IEEE Transactions on Automation Science and Engineering, vol. 13, no. 2, pp. 882–893, 2015.', metadata={'source': 'paper1.pdf'}), Document(page_content='[5] M. Pearce, B. Mutlu, J. Shah, and R. Radwin, “Optimizing Makespan and Ergonomics in Integrating Collaborative Robots into Manufac- turing Processes,” IEEE Transactions on Automation Science and Engineering, vol. 15, no. 4, pp. 1772–1784, 2018.\\n\\n[6] M. Javaid, A. Haleem, R. P. Singh, and R. Suman, “Substantial Capabilities of Robotics in Enhancing Industry 4.0 Implementation,” Cognitive Robotics, vol. 1, pp. 58–75, 2021.\\n\\n[7] K. Bogner, U. Pferschy, R. Unterberger, and H. Zeiner, “Optimised Scheduling in Human–Robot Collaboration–A Use Case in the As- sembly of Printed Circuit Boards,” International Journal of Production Research, vol. 56, no. 16, pp. 5522–5540, 2018.\\n\\n[8] U. Körner, K. Müller-Thur, T. Lunau, N. Dragano, P. Angerer, and A. Buchner, “Perceived Stress in Human–Machine Interaction in Mod- ern Manufacturing Environments—Results of a Qualitative Interview Study,” Stress and Health, vol. 35, no. 2, pp. 187–199, 2019.', metadata={'source': 'paper1.pdf'}), Document(page_content='[9] E. Matheson, R. Minto, E. G. Zampieri, M. Faccio, and G. Rosati, “Human–Robot Collaboration in Manufacturing Applications: A Re- view,” Robotics, vol. 8, no. 4, p. 100, 2019.\\n\\n[10] L. Floridi and M. Chiriatti, “GPT-3: Its Nature, Scope, Limits, and\\n\\nConsequences,” Minds and Machines, vol. 30, pp. 681–694, 2020.\\n\\n[11] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4 Technical Report,” arXiv preprint arXiv:2303.08774, 2023.\\n\\n[12] J. de Gea Fernández, D. Mronga, M. Günther, T. Knobloch, M. Wirkus, M. Schröer, M. Trampler, S. Stiene, E. Kirchner, V. Bargsten et al., “Multimodal Sensor-Based Whole-Body Control for Human–Robot Collaboration in Industrial Settings,” Robotics and Autonomous Sys- tems, vol. 94, pp. 102–119, 2017.', metadata={'source': 'paper1.pdf'}), Document(page_content='[13] D. Wei, L. Chen, L. Zhao, H. Zhou, and B. Huang, “A Vision- Based Measure of Environmental Effects on Inferring Human Intention During Human Robot Interaction,” IEEE Sensors Journal, vol. 22, no. 5, pp. 4246–4256, 2021.\\n\\n[14] H. Liu, T. Fang, T. Zhou, and L. Wang, “Towards Robust Human- Robot Collaborative Manufacturing: Multimodal Fusion,” IEEE Ac- cess, vol. 6, pp. 74 762–74 771, 2018.\\n\\n[15] W. Wang, R. Li, Y. Chen, and Y. Jia, “Human intention pre- diction in human-robot collaborative tasks,” in Companion of the 2018 ACM/IEEE international conference on human-robot interaction, 2018, pp. 279–280.\\n\\n[16] W. Wang, R. Li, Y. Chen, Z. M. Diekel, and Y. Jia, “Facilitating human–robot collaborative tasks by teaching-learning-collaboration from human demonstrations,” IEEE Transactions on Automation Sci- ence and Engineering, vol. 16, no. 2, pp. 640–653, 2018.', metadata={'source': 'paper1.pdf'}), Document(page_content='[17] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embod- ied Agents,” 2022.\\n\\n[18] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, “ProgPrompt: Generating Situated Robot Task Plans Using Large Language Models,” 2022.\\n\\n[19] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg, “Text2Motion: From Natural Language Instructions to Feasible Plans,” Autonomous Robots, vol. 47, no. 8, p. 1345–1365, Nov. 2023.\\n\\n[20] O. Mees, J. Borja-Diaz, and W. Burgard, “Grounding Language with\\n\\nVisual Affordances Over Unstructured Data,” 2023.\\n\\n[21] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, and C. Gan, “Building Cooperative Embodied Agents Modularly with Large Language Models,” 2024.', metadata={'source': 'paper1.pdf'}), Document(page_content='[21] H. Zhang, W. Du, J. Shan, Q. Zhou, Y. Du, J. B. Tenenbaum, T. Shu, and C. Gan, “Building Cooperative Embodied Agents Modularly with Large Language Models,” 2024.\\n\\n[22] A. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu, L. Takayama, F. Xia, J. Varley, Z. Xu, D. Sadigh, A. Zeng, and A. Majumdar, “Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners,” 2023.\\n\\n[23] G. Zheng, I. Kovalenko, K. Barton, and D. Tilbury, “Integrating Human Operators into Agent-Based Manufacturing Systems: A Table- Top Demonstration,” Procedia manufacturing, vol. 17, pp. 326–333, 2018.\\n\\n[24] UFACTORY, Nov 2023. [Online]. Available: https://www.ufactory.cc/ [25] OpenAI, “Speech to Text.” [Online]. Available: https://platform.\\n\\nopenai.com/docs/guides/speech-to-text\\n\\n[26] ——, “Text to Speech.” [Online]. Available: https://platform.openai.\\n\\ncom/docs/guides/text-to-speech\\n\\n[27] ——, “Function Calling.” [Online]. Available: https://platform.openai.', metadata={'source': 'paper1.pdf'}), Document(page_content='[26] ——, “Text to Speech.” [Online]. Available: https://platform.openai.\\n\\ncom/docs/guides/text-to-speech\\n\\n[27] ——, “Function Calling.” [Online]. Available: https://platform.openai.\\n\\ncom/docs/guides/function-calling\\n\\n[28] G. Jocher, “YOLOv5 by Ultralytics,” May 2020. [Online]. Available:\\n\\nhttps://github.com/ultralytics/yolov5\\n\\n[29] L. Xia, C. Li, C. Zhang, S. Liu, and P. Zheng, “Leveraging error- assisted fine-tuning large language models for manufacturing excel- lence,” Robotics and Computer-Integrated Manufacturing, vol. 88, p. 102728, 2024.\\n\\n[30] J. Lim, L. Pfeiffer, F. Ocker, B. Vogel-Heuser, and I. Kovalenko, “Ontology-Based Feedback to Improve Runtime Control for Multi- Agent Manufacturing Systems,” pp. 1–7, 2023.', metadata={'source': 'paper1.pdf'})]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Set your OpenAI API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Load PDF\n",
    "local_path = \"paper1.pdf\"\n",
    "loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "data = loader.load()\n",
    "\n",
    "# Preview first page\n",
    "print(data[0].page_content)\n",
    "\n",
    "# Define chunk size and overlap\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(chunks)\n",
    "\n",
    "# Generate embeddings using a pre-trained model\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Convert document chunks into a vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"local-rag\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define the retrieval prompt\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_db.as_retriever(), \n",
    "    llm=ChatOpenAI(model=\"gpt-4\"),\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.prompts import ChatPromptTemplate\n",
    "#from langchain.output_parsers import StrOutputParser\n",
    "#from langchain.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Define the RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create the processing chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The documents are about a framework for human-robot collaboration (HRC) in manufacturing assembly. The framework integrates Large Language Models (LLMs) with voice commands, robotic arms, and sensors to improve the safety and efficiency of interactions between humans and robots. It addresses challenges such as communication systems that require minimal robotics training, adaptability to manage changes and errors, and the integration of advanced technologies with human-centric design to improve usability. The framework's effectiveness is evaluated through a case study, which suggests potential improvements such as introducing feedback mechanisms and fine-tuning the LLM.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"What is this about?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cable shark assembly is composed of a housing, a wedge, a spring, and an end cap.\n"
     ]
    }
   ],
   "source": [
    "response2 = chain.invoke({\"question\": \"What is the cable shark assembly composed of?\"})\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The paper contributed a framework for human-robot collaborative assembly within a manufacturing environment. It aimed to improve the integration of natural language for human-robot interaction through a Large Language Model (LLM)-based approach. The paper validated this framework using a case study of the cable shark device assembly process. The results demonstrated the framework's ability to facilitate intuitive human-robot communication via voice commands with language variations and dynamically adapt to task variations and errors. The paper also analyzed the success rate of the robot understanding and correctly executing instructions based on the specificity of the instructions given.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"What was the contribution of this paper?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Future work includes testing the framework in various manufacturing scenarios, assessing initialization protocol, and enhancing robot adaptability and interaction.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"What was future work of this paper? summarize in 20 words or less\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first step in the assembly process is the housing assembly.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"What is the first step assembly process? summarize in 20 words or less\"})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
