{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set your OpenAI API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Initialize the embedding model with your OpenAI API key\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=\"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\")\n",
    "\n",
    "# Initialize the language model with your OpenAI API key\n",
    "llm = ChatOpenAI(model=\"gpt-4\", openai_api_key=\"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\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\jongh\\\\AppData\\\\Local\\\\Temp\\\\tmpfwu6zpb6'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m local_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSOP.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m loader \u001b[38;5;241m=\u001b[39m UnstructuredPDFLoader(file_path\u001b[38;5;241m=\u001b[39mlocal_path)\n\u001b[1;32m---> 13\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Preview first page\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mpage_content)\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:30\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\unstructured.py:89\u001b[0m, in \u001b[0;36mUnstructuredBaseLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlazy_load\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Document]:\n\u001b[0;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load file.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_process_elements(elements)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:73\u001b[0m, in \u001b[0;36mUnstructuredPDFLoader._get_elements\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_elements\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01munstructured\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpartition\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpdf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partition_pdf\n\u001b[1;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munstructured_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\documents\\elements.py:593\u001b[0m, in \u001b[0;36mprocess_metadata.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Element]:\n\u001b[1;32m--> 593\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    594\u001b[0m     call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    596\u001b[0m     regex_metadata: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m call_args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:626\u001b[0m, in \u001b[0;36madd_filetype.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[1;32m--> 626\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m     params \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    628\u001b[0m     include_metadata \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\file_utils\\filetype.py:582\u001b[0m, in \u001b[0;36madd_metadata.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    581\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Element]:\n\u001b[1;32m--> 582\u001b[0m     elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    583\u001b[0m     call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    584\u001b[0m     include_metadata \u001b[38;5;241m=\u001b[39m call_args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minclude_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\chunking\\dispatch.py:74\u001b[0m, in \u001b[0;36madd_chunking_strategy.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The decorated function is replaced with this one.\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# -- call the partitioning function to get the elements --\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m elements \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# -- look for a chunking-strategy argument --\u001b[39;00m\n\u001b[0;32m     77\u001b[0m call_args \u001b[38;5;241m=\u001b[39m get_call_args_applying_defaults(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\partition\\pdf.py:202\u001b[0m, in \u001b[0;36mpartition_pdf\u001b[1;34m(filename, file, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, include_metadata, metadata_filename, metadata_last_modified, chunking_strategy, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, date_from_file_object, starting_page_number, extract_forms, form_extraction_skip_tables, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m exactly_one(filename\u001b[38;5;241m=\u001b[39mfilename, file\u001b[38;5;241m=\u001b[39mfile)\n\u001b[0;32m    200\u001b[0m languages \u001b[38;5;241m=\u001b[39m check_language_args(languages \u001b[38;5;129;01mor\u001b[39;00m [], ocr_languages) \u001b[38;5;129;01mor\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpartition_pdf_or_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_page_breaks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_page_breaks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_table_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_table_structure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata_last_modified\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhi_res_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhi_res_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_images_in_pdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_image_block_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_image_block_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_image_block_to_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_from_file_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_from_file_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstarting_page_number\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstarting_page_number\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_forms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_forms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mform_extraction_skip_tables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\partition\\pdf.py:341\u001b[0m, in \u001b[0;36mpartition_pdf_or_image\u001b[1;34m(filename, file, is_image, include_page_breaks, strategy, infer_table_structure, ocr_languages, languages, metadata_last_modified, hi_res_model_name, extract_images_in_pdf, extract_image_block_types, extract_image_block_output_dir, extract_image_block_to_payload, date_from_file_object, starting_page_number, extract_forms, form_extraction_skip_tables, **kwargs)\u001b[0m\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m    331\u001b[0m         elements \u001b[38;5;241m=\u001b[39m _partition_pdf_or_image_with_ocr(\n\u001b[0;32m    332\u001b[0m             filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[0;32m    333\u001b[0m             file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    340\u001b[0m         )\n\u001b[1;32m--> 341\u001b[0m         out_elements \u001b[38;5;241m=\u001b[39m \u001b[43m_process_uncategorized_text_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[43melements\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_elements\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\partition\\pdf.py:920\u001b[0m, in \u001b[0;36m_process_uncategorized_text_elements\u001b[1;34m(elements)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m elements:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(el, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m el\u001b[38;5;241m.\u001b[39mcategory \u001b[38;5;241m==\u001b[39m ElementType\u001b[38;5;241m.\u001b[39mUNCATEGORIZED_TEXT:\n\u001b[1;32m--> 920\u001b[0m         new_el \u001b[38;5;241m=\u001b[39m \u001b[43melement_from_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mText\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m         new_el\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m el\u001b[38;5;241m.\u001b[39mmetadata\n\u001b[0;32m    922\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\partition\\text.py:294\u001b[0m, in \u001b[0;36melement_from_text\u001b[1;34m(text, coordinates, coordinate_system)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_possible_numbered_list(text):\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListItem(\n\u001b[0;32m    290\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m    291\u001b[0m         coordinates\u001b[38;5;241m=\u001b[39mcoordinates,\n\u001b[0;32m    292\u001b[0m         coordinate_system\u001b[38;5;241m=\u001b[39mcoordinate_system,\n\u001b[0;32m    293\u001b[0m     )\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mis_possible_narrative_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NarrativeText(\n\u001b[0;32m    296\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m    297\u001b[0m         coordinates\u001b[38;5;241m=\u001b[39mcoordinates,\n\u001b[0;32m    298\u001b[0m         coordinate_system\u001b[38;5;241m=\u001b[39mcoordinate_system,\n\u001b[0;32m    299\u001b[0m     )\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_possible_title(text):\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\partition\\text_type.py:80\u001b[0m, in \u001b[0;36mis_possible_narrative_text\u001b[1;34m(text, cap_threshold, non_alpha_threshold, languages, language_checks)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# NOTE(robinson): it gets read in from the environment as a string so we need to\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# cast it to a float\u001b[39;00m\n\u001b[0;32m     77\u001b[0m cap_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\n\u001b[0;32m     78\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNSTRUCTURED_NARRATIVE_TEXT_CAP_THRESHOLD\u001b[39m\u001b[38;5;124m\"\u001b[39m, cap_threshold),\n\u001b[0;32m     79\u001b[0m )\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mexceeds_cap_ratio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcap_threshold\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     81\u001b[0m     trace_logger\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot narrative. Text exceeds cap ratio \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcap_threshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# type: ignore # noqa: E501\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\partition\\text_type.py:276\u001b[0m, in \u001b[0;36mexceeds_cap_ratio\u001b[1;34m(text, threshold)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks the title ratio in a section of text. If a sufficient proportion of the words\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03mare capitalized, that can be indicated on non-narrative text (i.e. \"1A. Risk Factors\").\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    the function returns True\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# NOTE(robinson) - Currently limiting this to only sections of text with one sentence.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# The assumption is that sections with multiple sentences are not titles.\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43msentence_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text\u001b[38;5;241m.\u001b[39misupper():\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\partition\\text_type.py:225\u001b[0m, in \u001b[0;36msentence_count\u001b[1;34m(text, min_length)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msentence_count\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, min_length: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Checks the sentence count for a section of text. Titles should not be more than one\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m    sentence.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m        The min number of words a section needs to be for it to be considered a sentence.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m     count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\nlp\\tokenize.py:136\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39mCACHE_MAX_SIZE)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    135\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A wrapper around the NLTK sentence tokenizer with LRU caching enabled.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m     \u001b[43m_download_nltk_packages_if_not_present\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _sent_tokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\nlp\\tokenize.py:130\u001b[0m, in \u001b[0;36m_download_nltk_packages_if_not_present\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m tokenizer_available \u001b[38;5;241m=\u001b[39m check_for_nltk_package(\n\u001b[0;32m    126\u001b[0m     package_category\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, package_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (tokenizer_available \u001b[38;5;129;01mand\u001b[39;00m tagger_available):\n\u001b[1;32m--> 130\u001b[0m     \u001b[43mdownload_nltk_packages\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jongh\\projects\\llm-roboticarm\\.venv\\Lib\\site-packages\\unstructured\\nlp\\tokenize.py:88\u001b[0m, in \u001b[0;36mdownload_nltk_packages\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile() \u001b[38;5;28;01mas\u001b[39;00m tmp_file:\n\u001b[0;32m     87\u001b[0m     tgz_file \u001b[38;5;241m=\u001b[39m tmp_file\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m---> 88\u001b[0m     \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNLTK_DATA_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgz_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     file_hash \u001b[38;5;241m=\u001b[39m sha256_checksum(tgz_file)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_hash \u001b[38;5;241m!=\u001b[39m NLTK_DATA_SHA256:\n",
      "File \u001b[1;32mC:\\Python\\Python311\\Lib\\urllib\\request.py:251\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# Handle temporary file setup.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m--> 251\u001b[0m     tfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     tfp \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mNamedTemporaryFile(delete\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\jongh\\\\AppData\\\\Local\\\\Temp\\\\tmpfwu6zpb6'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Set your OpenAI API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# Load PDF\n",
    "local_path = \"SOP.pdf\"\n",
    "loader = UnstructuredPDFLoader(file_path=local_path)\n",
    "data = loader.load()\n",
    "\n",
    "# Preview first page\n",
    "print(data[0].page_content)\n",
    "\n",
    "# Define chunk size and overlap\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "chunks = text_splitter.split_documents(data)\n",
    "print(chunks)\n",
    "\n",
    "# Generate embeddings using a pre-trained model\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Convert document chunks into a vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks, \n",
    "    embedding=embedding_model,\n",
    "    collection_name=\"local-rag\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Define the retrieval prompt\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\"\n",
    ")\n",
    "\n",
    "# Set up the retriever\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_db.as_retriever(), \n",
    "    llm=ChatOpenAI(model=\"gpt-4\"),\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.prompts import ChatPromptTemplate\n",
    "#from langchain.output_parsers import StrOutputParser\n",
    "#from langchain.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "# Define the RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Create the processing chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | ChatOpenAI(model=\"gpt-4\")\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The documents are about a framework for human-robot collaboration (HRC) in manufacturing assembly. The framework integrates Large Language Models (LLMs) with voice commands, robotic arms, and sensors to improve the safety and efficiency of interactions between humans and robots. It addresses challenges such as communication systems that require minimal robotics training, adaptability to manage changes and errors, and the integration of advanced technologies with human-centric design to improve usability. The framework's effectiveness is evaluated through a case study, which suggests potential improvements such as introducing feedback mechanisms and fine-tuning the LLM.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"What is this about?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cable shark assembly is composed of a housing, a wedge, a spring, and an end cap.\n"
     ]
    }
   ],
   "source": [
    "response2 = chain.invoke({\"question\": \"What is the cable shark assembly composed of?\"})\n",
    "print(response2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The paper contributed a framework for human-robot collaborative assembly within a manufacturing environment. It aimed to improve the integration of natural language for human-robot interaction through a Large Language Model (LLM)-based approach. The paper validated this framework using a case study of the cable shark device assembly process. The results demonstrated the framework's ability to facilitate intuitive human-robot communication via voice commands with language variations and dynamically adapt to task variations and errors. The paper also analyzed the success rate of the robot understanding and correctly executing instructions based on the specificity of the instructions given.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"What was the contribution of this paper?\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Future work includes testing the framework in various manufacturing scenarios, assessing initialization protocol, and enhancing robot adaptability and interaction.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"What was future work of this paper? summarize in 20 words or less\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first step in the assembly process is the housing assembly.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"What is the first step assembly process? summarize in 20 words or less\"})\n",
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
